{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from util.data_management import concate_data, load_data_raw, display_data, data_to_fft, data_adjust_scale, data_reshape_for_train, data_split_to_chunk\n",
    "from util.data_generator_AE import DataGenerator\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn import metrics\n",
    "from easydict import EasyDict\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "    'dir_path' : r\"D:\\Anomaly-Dataset\", ## 1: C:\\Users\\VIP444\\Documents\\Anomaly-Dataset, 2: D:\\Anomaly-Dataset\\sar400_vibration_data\n",
    "    'is_normal' : True,\n",
    "    'is_train' : True,\n",
    "    'stop_idx' : 2,\n",
    "    'data_scale_fit' : True,\n",
    "    'data_scale_trans' : False,\n",
    "    'batch_size' : 16,\n",
    "    'split' : (0.9, 0.1),\n",
    "    'is_cache' : True,\n",
    "    'is_predict' : True,\n",
    "    'is_normalize' : False,\n",
    "    'is_lstm' : True,\n",
    "    'model_name' : 'LSTM'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataGenerator(\n",
    "    dataset_path=args.dir_path,\n",
    "    train_mode='Train',\n",
    "    batch_size=args.batch_size,\n",
    "    split=args.split,\n",
    "    is_train=args.is_train,\n",
    "    is_cache=args.is_cache,\n",
    "    is_normalize=args.is_normalize,\n",
    "    is_lstm=args.is_lstm\n",
    ")\n",
    "\n",
    "validation_dataset = DataGenerator(\n",
    "    dataset_path=args.dir_path,\n",
    "    train_mode='Validation',\n",
    "    batch_size=args.batch_size,\n",
    "    split=args.split,\n",
    "    is_train=args.is_train,\n",
    "    is_cache=args.is_cache,\n",
    "    is_normalize=args.is_normalize,\n",
    "    is_lstm=args.is_lstm\n",
    ")\n",
    "\n",
    "test_dataset = DataGenerator(\n",
    "    dataset_path=args.dir_path,\n",
    "    train_mode='Test',\n",
    "    batch_size=args.batch_size,\n",
    "    split=args.split,\n",
    "    is_train=False,\n",
    "    is_cache=args.is_cache,\n",
    "    is_normalize=args.is_normalize,\n",
    "    is_lstm=args.is_lstm,\n",
    "    is_predict=args.is_predict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset.data_paths))\n",
    "print(len(validation_dataset.data_paths))\n",
    "print(len(test_dataset.data_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ae_LSTM import autoencoder_model\n",
    "# from models.ae_Dense import autoencoder_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "if args.is_lstm:\n",
    "    input_shape = (1, 4)\n",
    "else:\n",
    "    input_shape = (4, )\n",
    "\n",
    "model = autoencoder_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss=Huber(), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "EXPERIMENT_DIR = f\"training_logs/{time.strftime('%Y-%m-%d-%H:%M:%S')-{args.model_name}}\"\n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "\n",
    "checkpoint_path = EXPERIMENT_DIR + \"/model/checkpoint.pt\"\n",
    "os.makedirs(EXPERIMENT_DIR + \"/model\", exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_mae', patience=7, verbose=1),\n",
    "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_mae', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_mae', factor=0.8, patience=6,verbose=1, min_lr=1e-3 * 1e-1),\n",
    "    CSVLogger(EXPERIMENT_DIR + '/train_log.csv', separator=',', append=True),\n",
    "    ]\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=(validation_dataset), epochs=epochs, batch_size=args.batch_size, callbacks=callbacks).history\n",
    "model.load_weights(checkpoint_path)\n",
    "model.save(EXPERIMENT_DIR + \"/model/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['loss'], 'b', label='Train_loss', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation_loss', linewidth=2)\n",
    "ax.set_title('Model Loss', fontsize=16)\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(test_dataset, verbose=1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset, verbose=1)\n",
    "print(predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset.is_predict = True\n",
    "predict_val = model.predict(validation_dataset, verbose=1)\n",
    "print(predict_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict.reshape(predict.shape[0], predict.shape[2])\n",
    "predict_val = predict_val.reshape(predict_val.shape[0], predict_val.shape[2])\n",
    "\n",
    "print(predict.shape)\n",
    "print(predict_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(validation_dataset.cache.keys())\n",
    "validation_data = np.array([row for key in keys for row in validation_dataset.cache[key][0]])\n",
    "\n",
    "validation_data = validation_data.reshape(validation_data.shape[0], validation_data.shape[2])\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = np.mean(np.power(validation_data - predict_val, 2), axis=1)\n",
    "mae = np.mean(np.abs(validation_data - predict_val), axis=1)\n",
    "\n",
    "y_valid = np.ones(len(validation_data))\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error' : mae, 'True_class': y_valid})\n",
    "\n",
    "precision_rt, recall_rt, threshold_rt = metrics.precision_recall_curve(error_df['True_class'], error_df['Reconstruction_error'])\n",
    "\n",
    "best_cnt_dic = abs(precision_rt - recall_rt)\n",
    "threshold = threshold_rt[np.argmin(best_cnt_dic)]\n",
    "\n",
    "print(precision_rt[np.argmin(best_cnt_dic)])\n",
    "print(recall_rt[np.argmin(best_cnt_dic)])\n",
    "print(threshold)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(threshold_rt, precision_rt[1:], label='Precision')\n",
    "plt.plot(threshold_rt, recall_rt[1:], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('model/model_loss_test.h5')\n",
    "\n",
    "# X_pred = model.predict(X_train)\n",
    "\n",
    "# X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2]) \n",
    "# X_pred = pd.DataFrame(X_pred)\n",
    "\n",
    "# Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2]) \n",
    "\n",
    "# scored = pd.DataFrame()\n",
    "# scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis=1)\n",
    "# # scored['Loss_mae'] = np.mean(X_pred-Xtrain, axis=1) \n",
    "\n",
    "# Threshold = 0.017\n",
    "\n",
    "# plt.figure(figsize=(16,9), dpi=80)\n",
    "# plt.title('Loss Distribution', fontsize=16)\n",
    "# plt.xlim([0,1])\n",
    "# sns.distplot(scored['Loss_mae'], kde= True, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = pd.DataFrame(predict)\n",
    "\n",
    "scored = pd.DataFrame()\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred), axis=1)\n",
    "scored['Threshold'] = threshold*2\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "print(scored)\n",
    "\n",
    "scored.to_csv(EXPERIMENT_DIR + '/test_log.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_val = pd.DataFrame(predict_val)\n",
    "\n",
    "scored_val = pd.DataFrame()\n",
    "scored_val['Loss_mae'] = np.mean(np.abs(X_pred_val-validation_data), axis=1)\n",
    "scored_val['Threshold'] = threshold*2\n",
    "scored_val['Anomaly'] = scored_val['Loss_mae'] > scored_val['Threshold']\n",
    "print(scored_val)\n",
    "scored_val.to_csv(EXPERIMENT_DIR + '/validation_log.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "606393872d2ae1b4d07a146e24c2bc65abd4ef04da8af9056b6661ebfe58ccf8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
