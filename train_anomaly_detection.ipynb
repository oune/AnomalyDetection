{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.data_management import load_data\n",
    "from util.feature import stack_feature, display_feature, get_mel_spectrogram_with_librosa\n",
    "from util.data_generator import DataGenerator\n",
    "from tensorflow.python.client import device_lib\n",
    "import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from util.metric import plot_confusion_matrix, plot_to_image\n",
    "import os\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "    'MODEL_NAME' : 'CNN',\n",
    "    'BATCH_SIZE' : 64,\n",
    "    'SHUFFLE' : True,\n",
    "    'SEED' : 555,\n",
    "    'NUM_CLASSES' : 1,\n",
    "    'INPUT_SHAPE' : (39, 532), # Width, Height\n",
    "    'LOG_PATH' : 'training_log',\n",
    "    'LEARNING_RATE' : 1e-3,\n",
    "    'NUM_MEL_BIN' : 39,\n",
    "    'SPLIT' : (0.9, 0.1, 0.0), # Train, Validation, Test\n",
    "    'EPOCH' : 100,\n",
    "    'NUM_LIMIT' : 4000\n",
    "})\n",
    "\n",
    "DATASET_PATH = r'C:\\Users\\VIP444\\Documents\\Github\\AnomalyDetection\\feature.json'\n",
    "\n",
    "try:\n",
    "    os.mkdir(args.LOG_PATH)\n",
    "except FileExistsError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_generator = DataGenerator(\n",
    "                                        DATASET_PATH,\n",
    "                                        seed=args.SEED,\n",
    "                                        split=args.SPLIT,\n",
    "                                        num_limit=args.NUM_LIMIT,\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=args.BATCH_SIZE,\n",
    "                                        caching=True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = audio_data_generator(train_mode='train')\n",
    "val_dataset = audio_data_generator(train_mode='val') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_dataset.element_spec[0].shape[1:]\n",
    "label_shape = train_dataset.element_spec[1].shape\n",
    "# label_names = np.array(tf.io.gfile.listdir(f'{DATASET_PATH}'))\n",
    "# num_labels = len(label_names)\n",
    "print(input_shape, label_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 39, 532, 32)       1184      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 39, 532, 32)       9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 39, 532, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 266, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 19, 266, 64)       18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 19, 266, 64)       36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 19, 266, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 133, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 133, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 133, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 133, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 133, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 66, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 66, 256)        295168    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 66, 256)        590080    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 66, 256)        590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 66, 256)        1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 33, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 33, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 33, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 33, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 33, 512)        2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 16, 512)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 16, 1024)       4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 16, 1024)       9438208   \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 16, 1024)       9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 16, 1024)       4096      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 31,717,697\n",
      "Trainable params: 31,713,665\n",
      "Non-trainable params: 4,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Dense, Conv2D, MaxPool2D, GlobalAvgPool2D, BatchNormalization\n",
    "from tensorflow.python.keras import Sequential\n",
    "input_shape = (39,532,4)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=input_shape, filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Conv2D(filters=1024, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=1024, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=1024, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalAvgPool2D())\n",
    "model.add(Dense(units=256, activation=\"relu\"))\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.strftime(\"%Y-%m-%d-%H%M%S\", time.localtime(time.time()))\n",
    "\n",
    "EXPERIMENT_DIR_NAME = f'{args.LOG_PATH}/{start_time}-{args.MODEL_NAME}'\n",
    "tensorboard = TensorBoard(log_dir=f'{EXPERIMENT_DIR_NAME}/tensorboard_logs', profile_batch=0)\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir=f'{EXPERIMENT_DIR_NAME}/tensorboard_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{EXPERIMENT_DIR_NAME}/params.txt', 'w') as file:\n",
    "    file.write(f'MODEL NAME : {args.MODEL_NAME}\\n')\n",
    "    file.write(f'DATASET_PATH : {DATASET_PATH}\\n')\n",
    "    file.write(f'BATCH SIZE : {args.BATCH_SIZE}\\n')\n",
    "    file.write(f'SHUFFLE : {args.SHUFFLE}\\n')\n",
    "    file.write(f'SEED : {args.SEED}\\n')\n",
    "    file.write(f'WIDTH HEIGHT : {args.INPUT_SHAPE}\\n')\n",
    "    file.write(f'SPLIT : {args.SPLIT}\\n')\n",
    "    file.write(f'LEARNING_RATE : {args.LEARNING_RATE}\\n')\n",
    "    file.write(f'NUM_MEL_BIN : {args.NUM_MEL_BIN}\\n')\n",
    "\n",
    "    for key in args.keys():\n",
    "        file.write(f'{key.upper()} : {args[key]}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "label_names = ['normal', 'abnoraml']\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict(val_dataset)\n",
    "    if num_labels > 2:\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "        # Calculate the confusion matrix.\n",
    "        cm = confusion_matrix(np.argmax(audio_data_generator.validation_labels, axis=1), test_pred)\n",
    "    else:\n",
    "        test_pred = np.round(tf.nn.sigmoid(test_pred_raw))\n",
    "        # Calculate the confusion matrix.\n",
    "        cm = confusion_matrix(audio_data_generator.validation_labels, test_pred)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=label_names)\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "\n",
    "if num_labels > 2:\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "else:\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=args.LEARNING_RATE),\n",
    "    loss=loss,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=7),\n",
    "    tensorboard,\n",
    "    tf.keras.callbacks.LambdaCallback(on_epoch_end = log_confusion_matrix),\n",
    "    tf.keras.callbacks.ModelCheckpoint(f'{EXPERIMENT_DIR_NAME}/model-{{epoch:02d}}', save_best_only=True, monitor='val_loss', mode='min'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=6, verbose=1, min_lr=args.LEARNING_RATE * 1e-1),\n",
    "    tf.keras.callbacks.CSVLogger(f'{EXPERIMENT_DIR_NAME}/train_log.csv', separator=',', append=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, \n",
    "                    validation_data=val_dataset, \n",
    "                    callbacks=callbacks, \n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(r'C:\\Users\\VIP444\\Documents\\Github\\AnomalyDetection\\training_log\\2022-03-15-215804-CNN\\model-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class EvaluateCSVLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filename, seperator=',', append=True):\n",
    "        self.filename = filename\n",
    "        self.seperator= seperator\n",
    "        self.append = append\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        self.csvfile = open(self.filename, 'w' if not self.append else 'a', newline='')\n",
    "        self.csvwriter = csv.writer(self.csvfile, delimiter=self.seperator,\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        print('test begin')\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        # write the contents of the dictionary logs to csv file\n",
    "        # sample content of logs {'batch': 0, 'size': 2, 'loss': -0.0, 'accuracy': 1.0}\n",
    "        pass\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        self.csvwriter.writerow(list(logs))\n",
    "        self.csvwriter.writerow(list(logs.values()))\n",
    "        self.csvfile.close()\n",
    "        print('test end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 26ms/step\n",
      "test begin\n",
      " 4/19 [=====>........................] - ETA: 0s - loss: 6.1963e-15 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIP444\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:5017: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 20ms/step - loss: 23.2156 - accuracy: 0.3457\n",
      "test end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[23.215585708618164, 0.34572169184684753]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_DIR_NAME = r'C:\\Users\\VIP444\\Documents\\Github\\AnomalyDetection\\training_log\\2022-03-15-215804-CNN'\n",
    "\n",
    "test_audio_generator = DataGenerator(\n",
    "                                        DATASET_PATH,\n",
    "                                        seed=args.SEED,\n",
    "                                        split=(0.0,0.0,1.0),\n",
    "                                        num_limit=args.NUM_LIMIT,\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=args.BATCH_SIZE,\n",
    "                                        caching=True,\n",
    "                                    )\n",
    "\n",
    "test_dataset = test_audio_generator(train_mode='test')\n",
    "predict_raw = model.predict(test_dataset,verbose=1)\n",
    "\n",
    "model.evaluate(\n",
    "    test_dataset,\n",
    "    callbacks=[EvaluateCSVLogger(f'{EXPERIMENT_DIR_NAME}/test_log.csv', seperator=',', append=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157\n"
     ]
    }
   ],
   "source": [
    "print(len(test_audio_generator.test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7310586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1157it [00:00, 2129.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "label_names = ['abnoraml', 'normal']\n",
    "\n",
    "def predict_logger(output, original_path,original, predict, label_names, error_only: bool = False, threshold=0.5):\n",
    "\n",
    "    with open(output, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        if original is not None :\n",
    "            csv_writer.writerow(['file path','file length', 'original label', 'predict label', *label_names])\n",
    "\n",
    "            for path, original_label, predict_logit in tqdm(zip(original_path, original, predict)):\n",
    "\n",
    "                if len(label_names) >= 3:\n",
    "                    predict_label_name = label_names[np.argmax(predict_logit, axis=0)]\n",
    "                else:\n",
    "                    predict_label_name = label_names[int(predict_logit > threshold)]\n",
    "\n",
    "                original_label_name = label_names[original_label]\n",
    "\n",
    "                if error_only and predict_label_name == original_label_name:\n",
    "                    continue\n",
    "\n",
    "                csv_writer.writerow([original_label_name, predict_label_name, *(list(predict_logit))])\n",
    "\n",
    "        else:\n",
    "\n",
    "            csv_writer.writerow(['file path','file length','predict label', *label_names])\n",
    "\n",
    "            for path, predict_logit in tqdm(zip(original_path, predict)):\n",
    "                basename = os.path.basename(path)\n",
    "\n",
    "                if len(label_names) >= 3:\n",
    "                    predict_label_name = label_names[np.argmax(predict_logit, axis=0)]\n",
    "                else:\n",
    "                    predict_label_name = label_names[int(predict_logit > threshold)]\n",
    "\n",
    "                csv_writer.writerow([basename,predict_label_name, *predict_logit])\n",
    "\n",
    "\n",
    "predict = tf.nn.sigmoid(predict_raw)\n",
    "\n",
    "y_true = np.array(test_audio_generator.test_labels)\n",
    "y_pred = np.array(predict)\n",
    "\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "# get the best threshold\n",
    "J = tpr - fpr\n",
    "ix = argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "\n",
    "print(best_thresh)\n",
    "\n",
    "predict_logger(f'{EXPERIMENT_DIR_NAME}/test-class2.csv', \n",
    "                test_audio_generator.test_paths, \n",
    "                test_audio_generator.test_labels,\n",
    "                predict,\n",
    "                label_names,\n",
    "                threshold=best_thresh\n",
    "                )\n",
    "\n",
    "with open(f'{EXPERIMENT_DIR_NAME}/params.txt', 'a') as file:\n",
    "    file.write(f'BEST TRESHOLD : {best_thresh}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from util.metric import plot_confusion_matrix, plot_to_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(['abnormal', 'normal']) >= 3:\n",
    "    test_pred = np.argmax(predict, axis=1)\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = confusion_matrix(np.argmax(test_audio_generator.test_labels, axis=1), test_pred)\n",
    "else:\n",
    "    test_pred = np.array(predict > best_thresh ,dtype=np.int64)\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = confusion_matrix(test_audio_generator.test_labels, test_pred)\n",
    "\n",
    "# Log the confusion matrix as an image summary.\n",
    "figure = plot_confusion_matrix(cm, class_names=['abnormal', 'normal'])\n",
    "cm_image = plot_to_image(figure)\n",
    "\n",
    "plt.imsave(f'{EXPERIMENT_DIR_NAME}/test-confusion.png', tf.squeeze(cm_image, axis=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(min(test_audio_generator.test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fcf736dfc80dfdb96a7496f377ec28d845f25eed5deec47b1f5d56ba2885b4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('general': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
