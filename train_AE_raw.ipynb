{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from util.data_management import concate_data, load_data_raw, display_data, data_to_fft, data_adjust_scale, data_reshape_for_train, data_split_to_chunk\n",
    "from util.data_generator_AE import DataGenerator\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn import metrics\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "    'dir_path' : r\"D:\\Anomaly-Dataset\\sar400_vibration_data\", ## 1: C:\\Users\\VIP444\\Documents\\Anomaly-Dataset, 2: D:\\Anomaly-Dataset\\sar400_vibration_data\n",
    "    'is_normal' : True,\n",
    "    'is_train' : True,\n",
    "    'stop_idx' : 2,\n",
    "    'data_scale_fit' : True,\n",
    "    'data_scale_trans' : False,\n",
    "    'batch_size' : 512,\n",
    "    'split' : (0.9, 0.1),\n",
    "    'is_cache' : True,\n",
    "    'is_normalize' : False,\n",
    "    'is_lstm' : True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_0, train_data_1, train_data_2, train_data_3, validation_data_0, validation_data_1, validation_data_2, validation_data_3 = load_data_raw(\n",
    "    dir_path=args.dir_path, \n",
    "    is_normal=args.is_normal, \n",
    "    is_train=args.is_train, \n",
    "    stop_idx=args.stop_idx\n",
    "    )\n",
    "\n",
    "test_data_0, test_data_1, test_data_2, test_data_3 = load_data_raw(\n",
    "    dir_path=args.dir_path,\n",
    "    is_normal=False,\n",
    "    is_train=False,\n",
    "    stop_idx=args.stop_idx\n",
    ")\n",
    "\n",
    "display_data(train_data_0, validation_data_0, test_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "train_data_0 = train_data_0[:len(test_data_0)]\n",
    "\n",
    "x = pd.DataFrame({\n",
    "    'x1': train_data_0.squeeze(-1),\n",
    "    'x2': test_data_0.squeeze(-1),\n",
    "    })\n",
    "\n",
    "scaler = RobustScaler()\n",
    "robust_df = scaler.fit_transform(x)\n",
    "robust_df = pd.DataFrame(robust_df, columns =['x1', 'x2'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standard_df = scaler.fit_transform(x)\n",
    "standard_df = pd.DataFrame(standard_df, columns =['x1', 'x2'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "minmax_df = scaler.fit_transform(x)\n",
    "minmax_df = pd.DataFrame(minmax_df, columns =['x1', 'x2'])\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\n",
    "ax1.set_title('Before Scaling')\n",
    "\n",
    "sns.kdeplot(x['x1'], ax = ax1, color ='r')\n",
    "sns.kdeplot(x['x2'], ax = ax1, color ='b')\n",
    "ax2.set_title('Robust Scaling')\n",
    "\n",
    "sns.kdeplot(robust_df['x1'], ax = ax2, color ='red')\n",
    "sns.kdeplot(robust_df['x2'], ax = ax2, color ='blue')\n",
    "ax3.set_title('Standard Scaling')\n",
    "\n",
    "sns.kdeplot(standard_df['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(standard_df['x2'], ax = ax3, color ='g')\n",
    "ax4.set_title('Min-Max Scaling')\n",
    "\n",
    "sns.kdeplot(minmax_df['x1'], ax = ax4, color ='black')\n",
    "sns.kdeplot(minmax_df['x2'], ax = ax4, color ='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('Scaler', StandardScaler())])\n",
    "\n",
    "X_train_0 = data_adjust_scale(pipeline, train_data_0, args.data_scale_fit)\n",
    "X_train_1 = data_adjust_scale(pipeline, train_data_1, args.data_scale_trans)\n",
    "X_train_2 = data_adjust_scale(pipeline, train_data_2, args.data_scale_trans)\n",
    "X_train_3 = data_adjust_scale(pipeline, train_data_3, args.data_scale_trans)\n",
    "\n",
    "X_validation_0 = data_adjust_scale(pipeline, validation_data_0, args.data_scale_trans)\n",
    "X_validation_1 = data_adjust_scale(pipeline, validation_data_1, args.data_scale_trans)\n",
    "X_validation_2 = data_adjust_scale(pipeline, validation_data_2, args.data_scale_trans)\n",
    "X_validation_3 = data_adjust_scale(pipeline, validation_data_3, args.data_scale_trans)\n",
    "\n",
    "X_test_0 = data_adjust_scale(pipeline, test_data_0, args.data_scale_trans)\n",
    "X_test_1 = data_adjust_scale(pipeline, test_data_1, args.data_scale_trans)\n",
    "X_test_2 = data_adjust_scale(pipeline, test_data_2, args.data_scale_trans)\n",
    "X_test_3 = data_adjust_scale(pipeline, test_data_3, args.data_scale_trans)\n",
    "\n",
    "display_data(X_train_0, X_validation_0, X_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = concate_data((X_train_0, X_train_1, X_train_2, X_train_3), 1)\n",
    "X_validation = concate_data((X_validation_0, X_validation_1, X_validation_2, X_validation_3), 1)\n",
    "X_test = concate_data((X_test_0, X_test_1, X_test_2, X_test_3), 1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train = data_reshape_for_train(X_train_0, X_train_1, X_train_2, X_train_3)\n",
    "X_validation = data_reshape_for_train(X_validation_0, X_validation_1, X_validation_2, X_validation_3)\n",
    "X_test = data_reshape_for_train(X_test_0, X_test_1, X_test_2, X_test_3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ae_LSTM import autoencoder_model\n",
    "# from models.ae_Dense import autoencoder_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "if args.is_lstm:\n",
    "    input_shape = (1, 4)\n",
    "else:\n",
    "    input_shape = (4, )\n",
    "\n",
    "model = autoencoder_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss=Huber(), metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "checkpoint_path = \"model/checkpoint.pt\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_mae', patience=7, verbose=1),\n",
    "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_mae', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_mae', factor=0.8, patience=6,verbose=1, min_lr=1e-3 * 1e-1),\n",
    "    CSVLogger('./train_log.csv', separator=',', append=True),\n",
    "    ]\n",
    "\n",
    "history = model.fit(X_train, X_train, epochs=epochs, batch_size=args.batch_size, callbacks=callbacks, validation_data=(X_validation, X_validation)).history\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "model.save(\"model/model.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
