{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from util.data_management import concate_data, load_data_raw, display_data, data_to_fft, data_adjust_scale, data_reshape_for_train, data_split_to_chunk\n",
    "from util.data_generator_AE import DataGenerator\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn import metrics\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "    'dir_path' : r\"D:\\Anomaly-Dataset\", ## 1: C:\\Users\\VIP444\\Documents\\Anomaly-Dataset, 2: D:\\Anomaly-Dataset\\sar400_vibration_data\n",
    "    'is_normal' : True,\n",
    "    'is_train' : True,\n",
    "    'stop_idx' : 2,\n",
    "    'data_scale_fit' : True,\n",
    "    'data_scale_trans' : False,\n",
    "    'batch_size' : 512,\n",
    "    'split' : (0.9, 0.1),\n",
    "    'is_cache' : True,\n",
    "    'is_normalize' : False,\n",
    "    'is_lstm' : True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataGenerator(\n",
    "    dataset_path=args.dir_path,\n",
    "    train_mode='Train',\n",
    "    batch_size=args.batch_size,\n",
    "    split=args.split,\n",
    "    is_train=args.is_train,\n",
    "    is_cache=args.is_cache,\n",
    "    is_normalize=args.is_normalize,\n",
    "    is_lstm=args.is_lstm\n",
    ")\n",
    "\n",
    "validation_dataset = DataGenerator(\n",
    "    dataset_path=args.dir_path,\n",
    "    train_mode='Validation',\n",
    "    batch_size=args.batch_size,\n",
    "    split=args.split,\n",
    "    is_train=args.is_train,\n",
    "    is_cache=args.is_cache,\n",
    "    is_normalize=args.is_normalize,\n",
    "    is_lstm=args.is_lstm\n",
    ")\n",
    "\n",
    "test_dataset = DataGenerator(\n",
    "    dataset_path=args.dir_path,\n",
    "    train_mode='Test',\n",
    "    batch_size=args.batch_size,\n",
    "    split=args.split,\n",
    "    is_train=False,\n",
    "    is_cache=args.is_cache,\n",
    "    is_normalize=args.is_normalize,\n",
    "    is_lstm=args.is_lstm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7816\n",
      "7816\n",
      "4276\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset.data_paths))\n",
    "print(len(validation_dataset.data_paths))\n",
    "print(len(test_dataset.data_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_0, train_data_1, train_data_2, train_data_3, validation_data_0, validation_data_1, validation_data_2, validation_data_3 = load_data_raw(\n",
    "    dir_path=args.dir_path, \n",
    "    is_normal=args.is_normal, \n",
    "    is_train=args.is_train, \n",
    "    stop_idx=args.stop_idx\n",
    "    )\n",
    "\n",
    "test_data_0, test_data_1, test_data_2, test_data_3 = load_data_raw(\n",
    "    dir_path=args.dir_path,\n",
    "    is_normal=False,\n",
    "    is_train=False,\n",
    "    stop_idx=args.stop_idx\n",
    ")\n",
    "\n",
    "display_data(train_data_0, validation_data_0, test_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "train_data_0 = train_data_0[:len(test_data_0)]\n",
    "\n",
    "x = pd.DataFrame({\n",
    "    'x1': train_data_0.squeeze(-1),\n",
    "    'x2': test_data_0.squeeze(-1),\n",
    "    })\n",
    "\n",
    "scaler = RobustScaler()\n",
    "robust_df = scaler.fit_transform(x)\n",
    "robust_df = pd.DataFrame(robust_df, columns =['x1', 'x2'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standard_df = scaler.fit_transform(x)\n",
    "standard_df = pd.DataFrame(standard_df, columns =['x1', 'x2'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "minmax_df = scaler.fit_transform(x)\n",
    "minmax_df = pd.DataFrame(minmax_df, columns =['x1', 'x2'])\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\n",
    "ax1.set_title('Before Scaling')\n",
    "\n",
    "sns.kdeplot(x['x1'], ax = ax1, color ='r')\n",
    "sns.kdeplot(x['x2'], ax = ax1, color ='b')\n",
    "ax2.set_title('Robust Scaling')\n",
    "\n",
    "sns.kdeplot(robust_df['x1'], ax = ax2, color ='red')\n",
    "sns.kdeplot(robust_df['x2'], ax = ax2, color ='blue')\n",
    "ax3.set_title('Standard Scaling')\n",
    "\n",
    "sns.kdeplot(standard_df['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(standard_df['x2'], ax = ax3, color ='g')\n",
    "ax4.set_title('Min-Max Scaling')\n",
    "\n",
    "sns.kdeplot(minmax_df['x1'], ax = ax4, color ='black')\n",
    "sns.kdeplot(minmax_df['x2'], ax = ax4, color ='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('Scaler', StandardScaler())])\n",
    "\n",
    "X_train_0 = data_adjust_scale(pipeline, train_data_0, args.data_scale_fit)\n",
    "X_train_1 = data_adjust_scale(pipeline, train_data_1, args.data_scale_trans)\n",
    "X_train_2 = data_adjust_scale(pipeline, train_data_2, args.data_scale_trans)\n",
    "X_train_3 = data_adjust_scale(pipeline, train_data_3, args.data_scale_trans)\n",
    "\n",
    "X_validation_0 = data_adjust_scale(pipeline, validation_data_0, args.data_scale_trans)\n",
    "X_validation_1 = data_adjust_scale(pipeline, validation_data_1, args.data_scale_trans)\n",
    "X_validation_2 = data_adjust_scale(pipeline, validation_data_2, args.data_scale_trans)\n",
    "X_validation_3 = data_adjust_scale(pipeline, validation_data_3, args.data_scale_trans)\n",
    "\n",
    "X_test_0 = data_adjust_scale(pipeline, test_data_0, args.data_scale_trans)\n",
    "X_test_1 = data_adjust_scale(pipeline, test_data_1, args.data_scale_trans)\n",
    "X_test_2 = data_adjust_scale(pipeline, test_data_2, args.data_scale_trans)\n",
    "X_test_3 = data_adjust_scale(pipeline, test_data_3, args.data_scale_trans)\n",
    "\n",
    "display_data(X_train_0, X_validation_0, X_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = concate_data((X_train_0, X_train_1, X_train_2, X_train_3), 1)\n",
    "X_validation = concate_data((X_validation_0, X_validation_1, X_validation_2, X_validation_3), 1)\n",
    "X_test = concate_data((X_test_0, X_test_1, X_test_2, X_test_3), 1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train = data_reshape_for_train(X_train_0, X_train_1, X_train_2, X_train_3)\n",
    "X_validation = data_reshape_for_train(X_validation_0, X_validation_1, X_validation_2, X_validation_3)\n",
    "X_test = data_reshape_for_train(X_test_0, X_test_1, X_test_2, X_test_3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 4)]            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1, 64)             17664     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1, 32)             8320      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1, 64)             24832     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 1, 4)              260       \n",
      "=================================================================\n",
      "Total params: 63,492\n",
      "Trainable params: 63,492\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models.ae_LSTM import autoencoder_model\n",
    "# from models.ae_Dense import autoencoder_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "if args.is_lstm:\n",
    "    input_shape = (1, 4)\n",
    "else:\n",
    "    input_shape = (4, )\n",
    "\n",
    "model = autoencoder_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 9/15 [=================>............] - ETA: 13:47 - loss: 1.1343 - mae: 0.9071"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "checkpoint_path = \"model/checkpoint.pt\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_mae', patience=7, verbose=1), \n",
    "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_mae', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_mae', factor=0.8, patience=6,verbose=1,min_lr=1e-3 * 1e-1),\n",
    "    CSVLogger('./train_log.csv', separator=',', append=True),\n",
    "    ]\n",
    "\n",
    "# history = model.fit(X_train, X_train, epochs=epochs, batch_size=args.batch_size, callbacks=callbacks, validation_data=(X_validation, X_validation)).history\n",
    "history = model.fit(train_dataset, validation_data=(validation_dataset), epochs=epochs, batch_size=args.batch_size, callbacks=callbacks).history\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "model.save(\"model/model_loss_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['loss'], 'b', label='Train_loss', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation_loss', linewidth=2)\n",
    "ax.set_title('Model Loss', fontsize=16)\n",
    "ax.set_ylabel('mae')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(test_dataset, verbose=1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset)\n",
    "print(predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.cache[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = concate_data((X_validation_0, X_validation_1, X_validation_2, X_validation_3), axis=1)\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], 1, X_validation.shape[1])\n",
    "\n",
    "# model = tf.keras.models.load_model('model/model_concate.h5')\n",
    "predictons_3d = model.predict(X_validation)\n",
    "predictions = predictons_3d.reshape(predictons_3d.shape[0], predictons_3d.shape[2])\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[2])\n",
    "mse = np.mean(np.power(X_validation - predictions, 2), axis=1)\n",
    "\n",
    "y_valid = np.ones(len(X_validation))\n",
    "\n",
    "error_df = pd.DataFrame({'Reconstruction_error':mse, 'True_class': y_valid})\n",
    "\n",
    "precision_rt, recall_rt, threshold_rt = metrics.precision_recall_curve(error_df['True_class'], error_df['Reconstruction_error'])\n",
    "\n",
    "best_cnt_dic = abs(precision_rt - recall_rt)\n",
    "threshold_fixed = threshold_rt[np.argmin(best_cnt_dic)]\n",
    "\n",
    "print(precision_rt[np.argmin(best_cnt_dic)])\n",
    "print(recall_rt[np.argmin(best_cnt_dic)])\n",
    "print(threshold_fixed)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(threshold_rt, precision_rt[1:], label='Precision')\n",
    "plt.plot(threshold_rt, recall_rt[1:], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model/model_concate.h5')\n",
    "\n",
    "X_pred = model.predict(X_train)\n",
    "\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2]) \n",
    "X_pred = pd.DataFrame(X_pred)\n",
    "\n",
    "Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2]) \n",
    "\n",
    "scored = pd.DataFrame()\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis=1)\n",
    "# scored['Loss_mae'] = np.mean(X_pred-Xtrain, axis=1) \n",
    "\n",
    "Threshold = 0.017\n",
    "\n",
    "plt.figure(figsize=(16,9), dpi=80)\n",
    "plt.title('Loss Distribution', fontsize=16)\n",
    "plt.xlim([0,1])\n",
    "sns.distplot(scored['Loss_mae'], kde= True, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(test_dataset)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred)\n",
    "\n",
    "print(X_pred)\n",
    "\n",
    "Threshold = 0.3\n",
    "\n",
    "# scored = pd.DataFrame()\n",
    "# Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\n",
    "# scored['Loss_mae'] = np.mean(np.abs(X_pred), axis=1)\n",
    "# scored['Threshold'] = Threshold\n",
    "# scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "# print(scored)\n",
    "\n",
    "# scored.to_csv('./test_log.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_validation = X_validation.reshape(X_validation.shape[0], 1, X_validation.shape[1])\n",
    "X_pred_val = model.predict(validation_dataset)\n",
    "X_pred_val = X_pred_val.reshape(X_pred_val.shape[0], X_pred_val.shape[2])\n",
    "X_pred_val = pd.DataFrame(X_pred_val)\n",
    "\n",
    "print(X_pred_val)\n",
    "\n",
    "# scored_val = pd.DataFrame()\n",
    "# Xvalidation = X_validation.reshape(X_validation.shape[0], X_validation.shape[2])\n",
    "# scored_val['Loss_mae'] = np.mean(np.abs(X_pred_val-Xvalidation), axis=1)\n",
    "# scored_val['Threshold'] = Threshold\n",
    "# scored_val['Anomaly'] = scored_val['Loss_mae'] > scored_val['Threshold']\n",
    "# print(scored_val)\n",
    "# scored_val.to_csv('./validation_log.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_train)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred)\n",
    "\n",
    "Threshold = 0.3\n",
    "\n",
    "scored = pd.DataFrame()\n",
    "Xtest = X_train.reshape(X_train.shape[0], X_train.shape[2])\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred), axis=1)\n",
    "scored['Threshold'] = Threshold\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "print(scored)\n",
    "\n",
    "scored.to_csv('./train_log.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "606393872d2ae1b4d07a146e24c2bc65abd4ef04da8af9056b6661ebfe58ccf8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
